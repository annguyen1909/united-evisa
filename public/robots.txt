# robots.txt â€” Optimized for search engine crawling
# This file allows search engines to crawl and index public pages
# while blocking private, transactional, and legacy URLs.

User-agent: *

# Allow everything by default so Google can discover pages, except
# explicitly private or transactional endpoints below.
Allow: /

# Private, admin, auth, API and processing endpoints (do not index)
Disallow: /api/
Disallow: /admin/
Disallow: /dashboard/
Disallow: /applications/
Disallow: /account/
Disallow: /profile
Disallow: /list
Disallow: /login
Disallow: /signup
Disallow: /register/
Disallow: /forgot-password/
Disallow: /reset-password/
Disallow: /processing/
Disallow: /private/
Disallow: /test/

# Block sensitive or non-canonical application subpaths (keep landing pages crawlable)
Disallow: /apply/payment
Disallow: /apply/confirmation
Disallow: /apply/status
Disallow: /apply/summary
Disallow: /apply/chargeback
Disallow: /apply/refunded
Disallow: /apply/documents
Disallow: /apply/passengers
Disallow: /apply?
Disallow: /apply?*

# Disallow Legacy Programmatic / Doorway Pages
# Note: /destination/* (singular) redirects to /destinations/* (plural) via next.config.ts
# Blocking the old singular paths prevents crawlers from indexing redirects
Disallow: /destination/
Disallow: /visa-for/

# Static assets (explicit allow helps some crawlers)
Allow: /_next/static/
Allow: /images/
Allow: /favicon.ico
Allow: /site.webmanifest

# Sitemaps (Absolute host URLs)
Sitemap: https://worldmaxxing.com/sitemap.xml

# Notes:
# - Avoid disallowing general site sections (e.g. /about, /list) unless they
#   contain duplicate or low-value content. Over-broad Disallow rules can
#   prevent Google from indexing many pages.
# - If you have a canonical domain, replace the Sitemap lines with full URLs.