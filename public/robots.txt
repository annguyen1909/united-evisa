# robots.txt — cleaned and permissive
# Keep in mind: for best results use absolute Sitemap URLs in production,
User-agent: *

# Allow everything by default so Google can discover pages, except
# explicitly private or transactional endpoints below.
Allow: /

# Private, admin, auth, API and processing endpoints (do not index)
Disallow: /api/
Disallow: /admin/
Disallow: /dashboard/
Disallow: /applications/
Disallow: /account/
Disallow: /login/
Disallow: /register/
Disallow: /forgot-password/
Disallow: /reset-password/
Disallow: /processing/
Disallow: /private/
Disallow: /test/

# Block sensitive or non-canonical application subpaths (keep landing pages crawlable)
Disallow: /apply/payment
Disallow: /apply/confirmation
Disallow: /apply/status
Disallow: /apply/summary
Disallow: /apply/chargeback
Disallow: /apply/refunded

# Static assets (explicit allow helps some crawlers)
Allow: /_next/static/
Allow: /images/
Allow: /favicon.ico
Allow: /site.webmanifest

# Sitemaps (relative paths — replace with absolute host URLs if possible)
Sitemap: https://worldmaxxing.com/sitemap.xml
Sitemap: https://worldmaxxing.com/sitemap-blog.xml
Sitemap: https://worldmaxxing.com/sitemap-destinations.xml

# Notes:
# - Avoid disallowing general site sections (e.g. /about, /list) unless they
#   contain duplicate or low-value content. Over-broad Disallow rules can
#   prevent Google from indexing many pages.
# - If you have a canonical domain, replace the Sitemap lines with full URLs.